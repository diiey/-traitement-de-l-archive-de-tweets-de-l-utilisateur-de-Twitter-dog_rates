{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Project: Wrangling and Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "In the cell below, gather **all** three pieces of data for this project and load them in the notebook. **Note:** the methods required to gather each data are different.\n",
    "1. Directly download the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import requests \n",
    "import os\n",
    "import tweepy \n",
    "from timeit import default_timer as timer\n",
    "from tweepy import OAuthHandler\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('twitter-archive-enhanced.csv' , sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the Requests library to download the tweet image prediction (image_predictions.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "r1 = requests.get(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(url1.split('/')[-1] , mode ='wb') as file:\n",
    "    file.write(r1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('image-predictions.tsv' , sep ='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the Tweepy library to query additional data via the Twitter API (tweet_json.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nous avons ici nos tokens pour nous identifier pour avoir accès à \n",
    "# notre API twitter\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "auth = tweepy.OAuthHandler(consumer_key , consumer_secret)\n",
    "auth.set_access_token(access_token , access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth , wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voici le code pour scraper les données twitter de weRateDogs\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# Tweet IDs for which to gather additional data via Twitter's API\n",
    "tweet_ids = df_1.tweet_id.values\n",
    "len(tweet_ids)\n",
    "\n",
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"Success\")\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(end - start)\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut aussi choisir de télécharger le fichier tweet_json \n",
    "# comme ceci\n",
    "url2 = 'https://video.udacity-data.com/topher/2018/November/5be5fb7d_tweet-json/tweet-json.txt'\n",
    "r2 = requests.get(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(url2.split('/')[-1] , mode ='wb') as file:\n",
    "    file.write(r2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_json(\"tweet-json.txt\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">de manière programmatique et visuelle des trois datasets , en sortant 8 problèmes de qualité au niveau des données et 2 problème de rangements ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problème de qualité\n",
    "   <ul><li>évaluation visuelle</li> </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le premier dataframme \"twitter-archive-enhanced.csv\" contient quasiment que des valeurs nulls (NAN) sur les colonnes suivants :\n",
    "<ol>\n",
    "    <li>in_reply_to_status_id</li>\n",
    "    <li>in_reply_to_user_id</li>\n",
    "    <li>retweeted_status_id</li>\n",
    "    <li>retweeted_status_user_id</li>\n",
    "    <li>retweeted_status_timestamp</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "     <li>évaluation programmatique</li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'évaluation programmatique on peut voir que :\n",
    ">Dans le premier dataset twitter-archive-enhanced.csv\n",
    "<ol>\n",
    "    <li> timestamp est de type  objet(string ou chaine ) alors cela devrait etre de type datetime  </li> \n",
    "    <li>nous avons les colonnes in_reply_to_status_id, in_reply_to_user_id , retweeted_status_id , retweeted_status_user_id , retweeted_status_timestamp qui ne contiennent quasiment que des valeurs nulles respectivement 2278,2278,2175,2175, 2175 valeurs nulles ;  nous avons aussi la colonne expanded_urls qui contient 59 valeurs nulles \n",
    "    <li> il y'a beaucoup de numerateurs et de dénominateurs qui contient des valeurs incohérentes comme des dénominateurs supérieures à 10 et des numérateurs avec des chiffres très grands qui sont problabement des nombres décimaux , donc des floats  </li> \n",
    "   \n",
    "</ol> \n",
    "\n",
    "Dans le second dataset image-predictions.tsv \n",
    "<ul>\n",
    "    <li> nous avons un doublon au niveau des valeurs de la colonne img_url </li>\n",
    "</ul> \n",
    "Dans le troixième dataset tweet-json.txt \n",
    "<ul>\n",
    "    <li> on supprime les retweets , on ne gardera que des tweets , ceux avec des images </li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problème d'ordre (Tidiness) \n",
    "\n",
    "<ul>\n",
    "    <li> évaluation visuelle </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">le premier dataset enfreins à la premiére règle d'ordre \"chaque variable doit constituer une colonne \" , dans la colonne timestamp on a la date , l'heure et le mois dans la meme colonne or on devrait les séparer en ayant une colonne date , une colonne heure et une colonne mois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>évaluation programmatique</li> \n",
    "     <li> les étapes du chien doggo , fluffer , pupper , puppo doivent une seule colonne  pour savoir si les chiens sont à quelle étape</li>\n",
    "</ul> \n",
    "toutes les tableauxs doivent former un seul tableau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.rating_numerator.sort_values() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.rating_denominator.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.jpg_url.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = pd.Series(list(df_1) + list(df_2) + list(df_3))\n",
    "all_columns[all_columns.duplicated()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Cleaning Data\n",
    "\n",
    ">Avant de procéder au nettoyage faisons d'abord des copies de chacun des ensembles de ces datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# faisons la copie de nos datasets \n",
    "cp_1 = df_1.copy() \n",
    "cp_2 = df_2.copy()\n",
    "cp_3 = df_3.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données manquantes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">les données manquantes que nous avions évalué  dans note premier dataset \"\"précédemment ne pas utiles dans notre analyse de données \n",
    "essayons de les supprimer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Supprimons les colonnes  in_reply_to_status_id, in_reply_to_user_id , retweeted_status_id , retweeted_status_user_id , retweeted_status_timestamp \n",
    "qui ne seront pas utiles à notre analyse car contient beaucoup de valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calulons le pourcentage des données manquants\n",
    "prtc_manquants = cp_1.isnull().sum()*100/cp_1.shape[0] \n",
    "prtc_manquants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des colonnes avec les données manquantes \n",
    "cp_1.drop( [\"in_reply_to_user_id\",\n",
    "          \"retweeted_status_timestamp\", \n",
    "          \"retweeted_status_id\",\n",
    "          \"retweeted_status_user_id\", \n",
    "          \"expanded_urls\"], axis = 1,  inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "list(cp_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des colonnes inutiles dans le dataframme cp_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Supprimmons les colonnes inutiles dans le dataset tweet_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des colonnes inutiles\n",
    "cp_3.drop([\"id_str\" , \"truncated\" , \"display_text_range\" , \"display_text_range\" ,\"entities\" , \"extended_entities\" , \"source\",\n",
    "        \"in_reply_to_status_id_str\" , \"in_reply_to_user_id\" , \n",
    "         \"in_reply_to_user_id_str\" , \"in_reply_to_screen_name\" , \"user\" , \"geo\" , \"coordinates\" , \"place\" , \"contributors\" ,\n",
    "         \"is_quote_status\" , \"favorited\" , \"retweeted\" , \"possibly_sensitive\" , \"possibly_sensitive_appealable\" ,\n",
    "         \"lang\" , \"quoted_status_id\" , \"quoted_status_id_str\",\"quoted_status\" ] , axis = 1  , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des colonnes inutiles dans le dataframme image_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nous allons supprimer les colonnes inutiles du dataset image_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des colonnes inutiles \n",
    "cp_2.drop(['img_num',\n",
    "                  'p2', \n",
    "                  'p2_conf',\n",
    "                  'p2_dog', \n",
    "                  'p3',\n",
    "                  'p3_conf',\n",
    "                  'p3_dog'] ,axis = 1,  inplace = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nettoyage des incohérences au niveau des numérateurs et dénominateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">On crééra une colonne moyenne afin d'éviter les nombres incohérents \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Creons  une  colonne  moyenne  en float qui calculera la moyenne des notations afin de faciliter les calculs .\n",
    "cp_1['moyenne'] = 10 * cp_1['rating_numerator'] / cp_1['rating_denominator'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_1.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage données dupliquées dans le dataset \"image-predictions.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nettoyage données dupliquées dans le dataset \"image-predictions.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supprimer l'image dupliquée\n",
    "cp_2 = cp_2.drop_duplicates(subset=['jpg_url'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_2.jpg_url.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage ordre  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Corrigeons le premier problème d'ordre trouvée sur l'évaluation visuelle \n",
    "Nous corrigerons aussi le second problème d'ordre qui est de former un seul tableau "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define \n",
    "> Extrayons la date et l'heure dans deux colonnes différentes dans la colonne timestamp  \n",
    "On convertit d'abord timestamp au format date puis on fait les séparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertissons timestamp au format date\n",
    "cp_1['timestamp'] = pd.to_datetime(cp_1['timestamp'])\n",
    "\n",
    "#extraire la date , le mois et l'année dans de nouvelles colonnes \n",
    "cp_1['date'] = cp_1['timestamp'].dt.date\n",
    "cp_1['hour'] = cp_1['timestamp'].dt.time\n",
    "cp_1['day'] = cp_1['timestamp'].dt.day\n",
    "\n",
    "#supprimer la colonne timestamp\n",
    "cp_1.drop( [\"timestamp\"], axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cp_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résolution des colonnes doggo , floofer , puppo et pupper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">nous allons fusionner les colonnes doggo , floofer,puppo et pupper dans un meme colonne nommé niveau chien \"level_dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#les données sur le type de chien est None, nous le changerons en  null et nous combinerons les 4 colonnes en une\n",
    "cp_1 = cp_1.replace('None', np.nan) \n",
    "#Combinons les colonnes et supprimons le dernier \n",
    "cp_1['dog_level'] = cp_1.doggo.combine_first(cp_1.floofer)\n",
    "cp_1['dog_level'] = cp_1.dog_level.combine_first(cp_1.pupper)\n",
    "cp_1['dog_level'] = cp_1.dog_level.combine_first(cp_1.puppo)\n",
    "cp_1.drop(['doggo' , 'floofer' , 'pupper' , 'puppo'] , axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusionnons les dataframmes df_1 , df_2 et df_3 afin qu'il ne forme qu'un seul dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Dans un premier temps , nous allons fusionner le dataset twitter-archive-enchanced et le dataset image _prediction dans un dataframme df_4 \n",
    "Enfin nous créerons un dataframme finale nommé df_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créons  un  dataframe qui fusionnera cp_1 et cp_2\n",
    "cp_4 = pd.merge(cp_1,cp_2, how = 'left', on = ['tweet_id'])\n",
    "# créons un dataframme final nommé df_twitter qui fusionnera les dataframmes cp_4 et cp_3\n",
    "df_twitter = pd.merge(cp_4, cp_3 , how = 'right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "Save gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ce fichier est trop volumineux donc entraine des problèmes d'espace pour cela on a préféré mis en commentaire \n",
    "df_twitter.to_csv('twitter_archive_master.csv', index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing and Visualizing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Dans cette partie nous sortirons 3 observations et  visualisation de nos données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 ère observation et visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.dog_level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dog_level = df_twitter.groupby('dog_level').filter(lambda x: len(x) >= 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dog_level['dog_level'].value_counts().plot(kind = 'barh')\n",
    "plt.title('Histogramme des niveau de chiens les plus présents dans le sondage')\n",
    "plt.xlabel('Nombre')\n",
    "plt.ylabel('les niveaux de chiens')\n",
    "\n",
    "fig = plt.gcf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 ème observation et visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=\"retweet_count\", y=\"moyenne\", data=df_twitter)\n",
    "plt.xlabel('Nombre de retweets')\n",
    "plt.ylabel('Moyenne')\n",
    "plt.title('Nombre de retweets par moyenne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 ième observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plusgrand_ind_moy = df_twitter['moyenne'].idxmax()\n",
    "plus_grand_moy = pd.DataFrame(df_twitter.loc[plusgrand_ind_moy,:])\n",
    "plus_grand_moy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pluspetit_ind_moy = df_twitter['moyenne'].idxmin()\n",
    "pluspetit_moy = pd.DataFrame(df_twitter.loc[pluspetit_ind_moy,:])\n",
    "pluspetit_moy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 ième observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df_twitter.copy()\n",
    "df_5.drop(df_5.index[5:],inplace =True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('les 5 chiens les plus aimés sont :')\n",
    "for index, row in df_5.iterrows():\n",
    "    print(\"chien :\", index+1)\n",
    "    print(\"Son nom est:\", row['name']),\n",
    "    print(\"il a \", row['favorite_count'], \"J'aime\")\n",
    "    print(\"il a \", row['retweet_count'] , \"retweets\")\n",
    "    print(\"il est au stade :\", row['dog_level'])\n",
    "    print(\"Voici comment on le présente:\", row['text'])\n",
    "    print(\"voici le lien de son image:\", row['jpg_url']) \n",
    "    print(                           )"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
